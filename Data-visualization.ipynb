{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c05182",
   "metadata": {},
   "source": [
    "# Data Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc4aeb1",
   "metadata": {},
   "source": [
    "Python is a very popular programming language and has very good libraries like _**NumPy**_ and _**Pandas**_ to manipulate data and _**Matplotlib**_ and _**Seaborn**_ to visualize the data. We can make many types of visualizations like Bar graphs, Line graphs, Boxplots, Histograms. \n",
    "\n",
    "Data visulalization is all about loading data, simplifying data, cleaning data, augmenting data (when it is not reach enough) and understand data on a more intuitive level. \n",
    "\n",
    "First of all, we need data in a plottable form to be able to plot data. NumPy and Pandas can be used for this purpose. We can efficinetly load, store, manipulate and export data using this libraries.\n",
    "\n",
    "Matplotlib and Seaborn are very popular Python plotting libraries. While Matplotlib API is relatively low-level, Seaborn API is hig-level and provides high-level graphics.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64fcfc",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84157c4e",
   "metadata": {},
   "source": [
    "We can load/import data from a csv file or an excel file using Pandas library. First, we need to import Pandas library. Then, we get our data using Pandas' read_csv function which takes the file path or url of the file as argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e802e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_url = 'https://drive.google.com/uc?id=1_0F4v5dven3QQ9QgmJieoxmTJi6mwjT5' \n",
    "dataset = pd.read_csv(file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f365daf7",
   "metadata": {},
   "source": [
    "Let's look at a small part of our dataset to figure out what kind of data we have. Rows include observations and columns include features. Rows have labels called indices or headers. It starts with zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(5)  # show the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b61212",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.tail(5)  # show the last 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33516e2",
   "metadata": {},
   "source": [
    "\n",
    "Each tabular view above is called Pandas' _**DataFrame**_ and we transfer some content of the data into the DataFrame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb56894",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset)  # type is Pandas Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8055eb",
   "metadata": {},
   "source": [
    "\n",
    "Now we can examine the data whether it is correctly loaded and valid. We also want to see the last rows to see if they have same format as the first rows.\n",
    "\n",
    "The data here shows you for example how much you will pay in interest over time or you can find out how much will be the total interest payment according to months for a particular car. But it is hard to see these informations just by looking at this table. That's why we need visualizations. \n",
    "\n",
    "We should verify our dataset. There are a couple of methods and attributes in Pandas library for it. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abbe98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape  # to see the shape of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da455b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes  # to check the column data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39234a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()  # gives you number of non-null values in the each column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f726f355",
   "metadata": {},
   "source": [
    "The last one is important to do because null values are not preferred for data analysis or visualization tasks. Here we have one null value in tje Interested Paid column. We will look at it in Missing Data section in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd1bd05",
   "metadata": {},
   "source": [
    "### Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed1419c",
   "metadata": {},
   "source": [
    "When we have a large dataset (which will be oftentimes), we can work on a smaller subset of it. For this, we can use following slicing techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['car_type']].tail()  # select one column, tail takes 5 default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59410d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['car_type', 'Principal Paid']].tail()  # select multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d836dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['car_type'].head()  # if we use single brackets, we get Pandas series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d763441",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset['car_type'].head())  # type is Pandas series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b67e41",
   "metadata": {},
   "source": [
    "We cannot select multiple columns using single brackets. \n",
    "\n",
    "With Pandas series, we can select rows using slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbe35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['car_type'][3:9]   # series[start_index:end_index] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f6e51",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf2be02",
   "metadata": {},
   "source": [
    "We can filter out the data using filtering techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d45d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['car_type'].value_counts()  # to see what kind of cars we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f7d94f",
   "metadata": {},
   "source": [
    "Here we can see that we have misspelling- Toyota Carolla instead of Toyota Corolla. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a76bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_filter=dataset['car_type']=='Toyota Sienna'  # produces Pandas series with True or False values\n",
    "car_filter.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea05fb",
   "metadata": {},
   "source": [
    "We can get observations only about Toyota Sienna using above car_filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8048bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[car_filter]  # shows observations related to Toyota Sienna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae76467",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sienna = dataset[car_filter]  # update the dataFrame after applying the filter\n",
    "dataset_sienna['car_type'].value_counts()  # shows that we have only the dataFrame of Toyota Sienna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5837c0f",
   "metadata": {},
   "source": [
    "### Renaming/Deleting Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3634494",
   "metadata": {},
   "source": [
    "Sometimes we may want to change the name of the columns. For example, when we see car_type column, we can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a2d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.car_type.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a156759",
   "metadata": {},
   "source": [
    "However, if we want to see Interest Paid column, we cannot use this way because we have a space in the name and this will create error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.Principal Paid.head()\n",
    "# dataset['Principal Paid'].head()  # this would work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c67ad3",
   "metadata": {},
   "source": [
    "Therefore, we may want to change the name of the columns. One approach to change the name of the columns is the dictionary substitution using rename method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_rn = dataset.rename(columns={'Starting Balance': 'starting_balance',\n",
    "                                  'Interest Paid' : 'interest_paid',\n",
    "                                  'Principal Paid': 'principal_paid',\n",
    "                                  'New Balance': 'new_balance'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3972b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_rn.columns  # to check whether the names of the columns changed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8797052",
   "metadata": {},
   "source": [
    "We can delete the columns using one of the two approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f782aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_dataset = dataset.drop(columns={'Repayment'})  # we can drop multiple columns\n",
    "del_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee5ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del del_dataset['Month'] \n",
    "del_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e75ffd",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf989a52",
   "metadata": {},
   "source": [
    "Before we graph data, we need to be sure there are no missing values like null, False, N/A, empty string. In Pandas, missing values are called NaN or None. \n",
    "\n",
    "If we have one of them in a row, then we should remove that row or fill in the missing data with a reasonable value. The _**isna**_ and _**isnull**_ methods can be used to indicate where the values in the DataFrame are missing. They are exaclty same methods. They return True if we have a missing value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ca0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_missing = dataset['Interest Paid'].isnull()  # produces Panda series with True and False\n",
    "dataset[interest_missing]  # shows the location of the missing value (we filter it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b6167",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[~interest_missing][32:37]  # shows the rows which don't have any missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58d5972",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Interest Paid'].isnull().sum()  # counts the number of missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21672e",
   "metadata": {},
   "source": [
    "We can remove missing values by using the _**dropna**_ method. However, deleting the entire row may not be the best strategy for our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a138ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_mv = dataset.dropna(how='any')  # drop entire row if it contains any NaNs \n",
    "drop_mv['Interest Paid'][32:37]  # shows that it drops the row 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278225c0",
   "metadata": {},
   "source": [
    "We can fill in the NaN value with a value. For this, we can use _**fillna()**_ method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c6eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mv = dataset.fillna(0)  # filling in the nan value with zero, it is not recommended\n",
    "fill_mv['Interest Paid'][32:37]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a948dc",
   "metadata": {},
   "source": [
    "Another approach is to fill in with the _**bfill()**_ method. Back fill looks for the next rows value and changes NaN with that value. This is commanly done when we have time series data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9974809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bfill_mv = dataset.bfill()  # looks for the next row and takes that value for NaN \n",
    "bfill_mv['Interest Paid'][32:37]  # shows that the values of 35 and 34 are same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
